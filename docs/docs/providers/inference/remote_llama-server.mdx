---
description: "llama.cpp inference provider for connecting to llama.cpp servers with OpenAI-compatible API."
sidebar_label: Remote - Llama-Server
title: remote::llama-server
---

# remote::llama-server

## Description

llama.cpp inference provider for connecting to llama.cpp servers with OpenAI-compatible API.

## Configuration

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `allowed_models` | `list[str] \| None` | No |  | List of models that should be registered with the model registry. If None, all models are allowed. |
| `refresh_models` | `bool` | No | False | Whether to refresh models periodically from the provider |
| `api_key` | `SecretStr \| None` | No |  | Authentication credential for the provider |
| `base_url` | `HttpUrl \| None` | No | https://localhost:8080/v1 | The URL for the Llama cpp server |

## Sample Configuration

```yaml
base_url: https://localhost:8080/v1
```
